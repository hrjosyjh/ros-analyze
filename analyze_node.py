#!/usr/bin/env python3
""".
ROS 2 Node Log Analyzer
ì‚¬ìš©ë²•:
    1. ì „ì²´ ë…¸ë“œ ìš”ì•½:
       python3 analyze_node.py launch.log

    2. íŠ¹ì • ë…¸ë“œ ìƒì„¸ ë¶„ì„:
       python3 analyze_node.py launch.log --node motor_driver

    3. ì—ëŸ¬ê°€ ë°œìƒí•œ ë…¸ë“œë§Œ ë³´ê¸°:
       python3 analyze_node.py launch.log --errors-only
"""

import re
import sys
import argparse
from datetime import datetime
from collections import defaultdict, Counter

from log_parser import parse_datetime_arg
from i18n import set_lang, t

# ==============================================================================
#  Log Parsing Logic
# ==============================================================================

# Regex patterns for different ROS 2 log formats
# Format A: [INFO] [timestamp] [node_name]: message
# Format B: 123456.789 [INFO] [node_name]: message (Launch prefix)
# Format C: 123456.789 [node_name] [INFO] ...
RE_LAUNCH_PREFIX = re.compile(r'^(\d+\.\d+)\s+\[(INFO|ERROR|WARN|DEBUG|FATAL)\]\s+\[([^\]]+)\]\s*(.*)$')
RE_ROS2_FORMAT   = re.compile(r'^(\d+\.\d+)\s+\[([^\]]+)\]\s+\[(INFO|ERROR|WARN|DEBUG|FATAL)\]\s*(.*)$')
RE_SIMPLE_NODE   = re.compile(r'^(\d+\.\d+)\s+\[([^\]]+)\]\s*(.*)$')

# ANSI Color codes for level inference (if text level is missing)
ANSI_COLOR_MAP = {
    '31': 'ERROR', '1;31': 'ERROR',  # Red
    '33': 'WARN',  '1;33': 'WARN',   # Yellow
    '32': 'DEBUG', '1;32': 'DEBUG',  # Green
}

def parse_line(line):
    """
    Parses a single log line and returns (timestamp, node_name, level, message).
    """
    line = line.strip()
    
    # Try Format A (Launch Prefix with Level first)
    m = RE_LAUNCH_PREFIX.match(line)
    if m:
        ts, level, node, msg = m.groups()
        return float(ts), node.strip(), level, msg

    # Try Format B (Node first, then Level)
    m = RE_ROS2_FORMAT.match(line)
    if m:
        ts, node, level, msg = m.groups()
        return float(ts), node.strip(), level, msg

    # Try Format C (Simple Node prefix, check for ANSI colors or assume INFO)
    m = RE_SIMPLE_NODE.match(line)
    if m:
        ts, node, content = m.groups()
        level = 'INFO'
        
        # Check for ANSI color codes to infer level
        if '\x1b[' in line:
            for code, mapped_level in ANSI_COLOR_MAP.items():
                if f'\x1b[{code}m' in line:
                    level = mapped_level
                    break
        
        # Clean message (remove ANSI codes for readability)
        msg = re.sub(r'\x1b\[[0-9;]*m', '', content)
        return float(ts), node.strip(), level, msg

    return None

# ==============================================================================
#  Analysis Classes
# ==============================================================================

class NodeStats:
    def __init__(self, name):
        self.name = name
        self.count = 0
        self.levels = defaultdict(int)
        self.first_ts = float('inf')
        self.last_ts = float('-inf')
        self.error_samples = []  # Store unique error messages
        self.activity_timeline = defaultdict(int) # Bucketized by minute

    def add(self, ts, level, msg):
        self.count += 1
        self.levels[level] += 1
        self.first_ts = min(self.first_ts, ts)
        self.last_ts = max(self.last_ts, ts)
        
        # Timeline (1-minute buckets)
        bucket = int(ts // 60)
        self.activity_timeline[bucket] += 1

        # Store Error Samples (Simple deduplication)
        if level in ['ERROR', 'FATAL', 'WARN']:
            # Remove timestamps/numbers from msg to group similar errors
            clean_msg = re.sub(r'\d+', 'N', msg[:100]) 
            if len(self.error_samples) < 50: # Limit memory usage
                self.error_samples.append((ts, level, msg, clean_msg))

    def get_duration(self):
        if self.count == 0: return 0
        return self.last_ts - self.first_ts

    def get_error_count(self):
        return self.levels['ERROR'] + self.levels['FATAL'] + self.levels['WARN']

# ==============================================================================
#  Reporting Functions
# ==============================================================================

def print_global_summary(nodes, errors_only=False):
    print(f"\n{'='*90}")
    print(f" {t('ROS 2 ì‹œìŠ¤í…œ ë¶„ì„ ë³´ê³ ì„œ', 'ROS 2 System Analysis Report')}")
    print(f"{'='*90}")
    print(f" {t('ë…¸ë“œëª…', 'Node Name'):<40} | {t('í•©ê³„', 'Total'):>8} | {t('ì—ëŸ¬', 'Errors'):>6} | {t('ê²½ê³ ', 'Warn'):>6} | {'FPS':>5} | {t('ê¸°ê°„', 'Duration')}")
    print(f"{'-'*90}")

    # Sort by error count (descending) then total count
    sorted_nodes = sorted(nodes.values(), key=lambda x: (x.get_error_count(), x.count), reverse=True)

    for n in sorted_nodes:
        err_count = n.levels['ERROR'] + n.levels['FATAL']
        warn_count = n.levels['WARN']
        
        if errors_only and (err_count + warn_count) == 0:
            continue

        duration = n.get_duration()
        fps = n.count / duration if duration > 0 else 0
        dur_str = f"{duration:.1f}s"
        
        # Highlight high error nodes
        prefix = "ğŸ”´" if err_count > 0 else "  "
        
        print(f" {prefix}{n.name:<38} | {n.count:>8,} | {err_count:>6} | {warn_count:>6} | {fps:>5.1f} | {dur_str}")

    print(f"{'='*90}\n")


def print_node_detail(node_stats):
    if not node_stats:
        print(t("ë…¸ë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", "Node not found."))
        return

    n = node_stats
    duration = n.get_duration()

    print(f"\n{'='*80}")
    print(f" {t('ìƒì„¸ ë¶„ì„', 'Detailed Analysis')}: {n.name}")
    print(f"{'='*80}")
    print(f" - {t('ì´ ë¡œê·¸ ìˆ˜', 'Total Logs')}: {n.count:,}")
    print(f" - {t('ì²« ë¡œê·¸', 'First Log')} : {datetime.fromtimestamp(n.first_ts).strftime('%Y-%m-%d %H:%M:%S')}")
    print(f" - {t('ë§ˆì§€ë§‰ ë¡œê·¸', 'Last Log')}  : {datetime.fromtimestamp(n.last_ts).strftime('%Y-%m-%d %H:%M:%S')}")
    print(f" - {t('ê¸°ê°„', 'Duration')}  : {duration:.2f} {t('ì´ˆ', 'seconds')}")
    print(f" - {t('ë¡œê·¸ ì²˜ë¦¬ìœ¨', 'Log Rate')}  : {n.count / duration:.1f} lines/sec" if duration > 0 else f" - {t('ë¡œê·¸ ì²˜ë¦¬ìœ¨', 'Log Rate')} : N/A")

    print(f"\n [{t('ë¡œê·¸ ë ˆë²¨ ë¶„í¬', 'Log Level Distribution')}]")
    for level, count in n.levels.items():
        bar = "â–ˆ" * int((count / n.count) * 50)
        print(f"   {level:<5} : {count:>6,} {bar}")

    # Timeline visualization
    print(f"\n [{t('í™œë™ íƒ€ì„ë¼ì¸ (ë¶„ë‹¹ ë¡œê·¸ ìˆ˜)', 'Activity Timeline (Logs per Minute)')}]")
    if n.activity_timeline:
        min_bucket = min(n.activity_timeline.keys())
        max_bucket = max(n.activity_timeline.keys())
        
        # Normalize for bar chart
        max_val = max(n.activity_timeline.values())
        
        for b in range(min_bucket, max_bucket + 1):
            val = n.activity_timeline.get(b, 0)
            if val == 0: continue
            
            ts_str = datetime.fromtimestamp(b * 60).strftime('%H:%M')
            bar_len = int((val / max_val) * 40)
            print(f"   {ts_str} : {val:>5} {'#' * bar_len}")

    # Error Analysis
    err_count = n.levels['ERROR'] + n.levels['FATAL'] + n.levels['WARN']
    if err_count > 0:
        print(f"\n [{t('ìƒìœ„ ì—ëŸ¬/ê²½ê³  íŒ¨í„´', 'Top Error/Warning Patterns')}]")

        # Group by "cleaned" message
        patterns = Counter([x[3] for x in n.error_samples])

        for clean_msg, count in patterns.most_common(5):
            # Find original message for this pattern
            example = next(x[2] for x in n.error_samples if x[3] == clean_msg)
            print(f"   ({count} {t('íšŒ ë°œìƒ', 'occurrences')})")
            print(f"   â””â”€â”€ {example[:120]}...")
            print()
    else:
        print(t("\n âœ… ì—ëŸ¬ ë˜ëŠ” ê²½ê³ ê°€ ì—†ìŠµë‹ˆë‹¤.", "\n âœ… No Errors or Warnings detected."))
    print("\n")


# ==============================================================================
#  Main Execution
# ==============================================================================

def main():
    parser = argparse.ArgumentParser(description="Analyze ROS 2 Log Files")
    parser.add_argument("logfile", help="Path to the log file (e.g., launch.log)")
    parser.add_argument("--node", help="Specific node name to analyze (substring match)")
    parser.add_argument("--errors-only", action="store_true", help="Only show nodes with errors in summary")
    parser.add_argument('--from', dest='time_from', default=None,
                        help='Start time / ë¶„ì„ ì‹œì‘ ì‹œê° (e.g. "2026-01-27", "2026-01-27 09:00", "09:00")')
    parser.add_argument('--to', dest='time_to', default=None,
                        help='End time / ë¶„ì„ ì¢…ë£Œ ì‹œê° (e.g. "2026-01-28", "2026-01-27 18:00", "18:00")')
    parser.add_argument('--lang', '-L', choices=['ko', 'en'], default='ko',
                        help='Output language / ì¶œë ¥ ì–¸ì–´ (ko: í•œêµ­ì–´, en: English) [default: ko]')

    args = parser.parse_args()
    set_lang(args.lang)

    # ì‹œê°„ ë²”ìœ„ íŒŒì‹±
    ts_from = None
    ts_to = None
    if args.time_from:
        try:
            ts_from = parse_datetime_arg(args.time_from)
        except ValueError as e:
            parser.error(str(e))
    if args.time_to:
        try:
            ts_to = parse_datetime_arg(args.time_to)
        except ValueError as e:
            parser.error(str(e))

    nodes = {}
    total_lines = 0

    print(t(f"{args.logfile} ë¶„ì„ ì¤‘...", f"Analyzing {args.logfile}..."))
    if ts_from is not None or ts_to is not None:
        from_str = datetime.fromtimestamp(ts_from).strftime('%Y-%m-%d %H:%M:%S') if ts_from else t('(ì²˜ìŒ)', '(start)')
        to_str = datetime.fromtimestamp(ts_to).strftime('%Y-%m-%d %H:%M:%S') if ts_to else t('(ë)', '(end)')
        print(f"  {t('ì‹œê°„ ë²”ìœ„', 'Time range')}: {from_str} ~ {to_str}")

    try:
        with open(args.logfile, 'r', encoding='utf-8', errors='replace') as f:
            for line in f:
                total_lines += 1
                result = parse_line(line)

                if result:
                    ts, node_name, level, msg = result

                    # ì‹œê°„ ë²”ìœ„ í•„í„°
                    if ts_from is not None and ts < ts_from:
                        continue
                    if ts_to is not None and ts > ts_to:
                        continue

                    if node_name not in nodes:
                        nodes[node_name] = NodeStats(node_name)

                    nodes[node_name].add(ts, level, msg)

                if total_lines % 100000 == 0:
                    sys.stdout.write(f"\r{t('ì²˜ë¦¬ ì¤‘', 'Processed')} {total_lines:,} {t('ì¤„...', 'lines...')}")
                    sys.stdout.flush()

    except FileNotFoundError:
        print(t(f"ì—ëŸ¬: '{args.logfile}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", f"Error: File '{args.logfile}' not found."))
        sys.exit(1)

    print(f"\r{t('ì²˜ë¦¬ ì™„ë£Œ', 'Processed')} {total_lines:,} {t('ì¤„. ì™„ë£Œ.', 'lines. Complete.')}      ")

    if args.node:
        # Find partial matches
        matched = [n for n in nodes.values() if args.node in n.name]
        if not matched:
            print(t(f"'{args.node}'ì— ë§¤ì¹­ë˜ëŠ” ë…¸ë“œê°€ ì—†ìŠµë‹ˆë‹¤.", f"No nodes matching '{args.node}' found."))
        else:
            for n in matched:
                print_node_detail(n)
    else:
        print_global_summary(nodes, args.errors_only)

if __name__ == "__main__":
    main()